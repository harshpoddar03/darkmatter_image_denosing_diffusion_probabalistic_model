{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "directory = './Samples'\n",
    "numpy_list = []\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        image_np = np.load(file_path)\n",
    "\n",
    "        # Assuming the loaded image shape is (1, 150, 150), i.e., grayscale with a single channel\n",
    "        if image_np.shape == (1, 150, 150):\n",
    "            # Remove the channel dimension\n",
    "            image_np = np.squeeze(image_np, axis=0)\n",
    "            # Convert grayscale to RGB by repeating the single channel 3 times\n",
    "            image_np = np.repeat(image_np[np.newaxis, :, :], 3, axis=0)\n",
    "\n",
    "        # Append the RGB numpy array directly to the list\n",
    "        numpy_list.append(image_np)\n",
    "\n",
    "# Now, `numpy_list` contains all your images as numpy arrays of shape (3, 150, 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "numpy_list = np.array(numpy_list)\n",
    "print(numpy_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "tensor_list = [torch.tensor(image_np, dtype=torch.float32) for image_np in numpy_list]\n",
    "\n",
    "# Normalize the tensors to have values between 0 and 1 if they aren't already\n",
    "tensor_list = [tensor / 255. for tensor in tensor_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import Compose, Resize, Lambda\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Assuming images_np is your numpy array with shape (10000, 3, 150, 150)\n",
    "# For demonstration, creating a dummy numpy array with random values\n",
    "\n",
    "# Step 1: Convert numpy array to PyTorch tensor\n",
    "images_tensor = tensor_list\n",
    "\n",
    "# Step 2: Define a transform to resize images to 128x128\n",
    "transform = Compose([\n",
    "    Lambda(lambda x: TF.to_pil_image(x)),\n",
    "    Resize((128, 128)),  # Convert to PIL Image to use torchvision's Resize                  # Resize image to 128x128\n",
    "    Lambda(lambda x: TF.to_tensor(x)),     # Convert back to tensor\n",
    "])\n",
    "\n",
    "# Apply the transform to each image in the tensor\n",
    "images_tensor_resized = torch.stack([transform(img) for img in images_tensor])\n",
    "\n",
    "# Step 3: Create a Dataset and DataLoader\n",
    "dataset = TensorDataset(images_tensor_resized)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "model = Unet(\n",
    "    dim=64,\n",
    "    dim_mults=(1, 2, 4, 8),\n",
    "    flash_attn=True\n",
    ")\n",
    "# Initialize the GaussianDiffusion process\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size=128,\n",
    "    timesteps=1000  # Number of steps\\\n",
    ")\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)  # Move model to the GPU\n",
    "diffusion = diffusion.to(device)  # Ensure diffusion model is also on the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 313/313 [13:35<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Loss = 0.08111970683637137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [01:59<00:00,  8.36it/s]\n",
      "Training: 100%|██████████| 313/313 [13:39<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Loss = 0.011547146545062526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [02:01<00:00,  8.26it/s]\n",
      "Training: 100%|██████████| 313/313 [13:32<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Loss = 0.0066943497035485775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [02:02<00:00,  8.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training loop (simplified example)\n",
    "# num_epochs = 3  # Set the number of epochs as needed\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in data_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         images = batch[0]  # Extract images tensor from the batch\n",
    "#         loss = diffusion(images)  # Compute loss\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update model parameters\n",
    "#     print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n",
    "\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#run on gpu\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "num_epochs = 3  # Set the number of epochs as needed\n",
    "save_image_interval = 1  # How often to save generated images (every N epochs)\n",
    "\n",
    "# Directory for saving generated images\n",
    "generated_images_dir = './results'\n",
    "os.makedirs(generated_images_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0  # To accumulate loss over the epoch\n",
    "    for batch in tqdm(data_loader,desc=\"Training\",total=len(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        images = batch[0].to(device)  # Extract images tensor from the batch\n",
    "        loss = diffusion(images).to(device)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1}: Avg Loss = {avg_epoch_loss}\")\n",
    "\n",
    "    # Periodically generate and save images to monitor progress\n",
    "    if (epoch + 1) % save_image_interval == 0:\n",
    "        with torch.no_grad():  # No need to track gradients for image generation\n",
    "            sampled_images = diffusion.sample(batch_size=16)\n",
    "            for i, img in enumerate(sampled_images):\n",
    "                img_path = os.path.join(generated_images_dir, f'epoch_{epoch + 1}_image_{i}.png')\n",
    "                save_image(img, img_path)\n",
    "\n",
    "# After training, it's also helpful to evaluate the model using more comprehensive metrics like FID score.\n",
    "# Ensure you have a separate script or process to calculate FID against a set of real images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, sample images from the model\n",
    "# sampled_images = diffusion.sample(batch_size=8)\n",
    "# print(f\"Sampled images shape: {sampled_images.shape}\")  # Should be (8, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.utils import save_image\n",
    "\n",
    "# # Assume `sampled_images` is your tensor of generated images\n",
    "# for i, img in enumerate(sampled_images):\n",
    "#     # Define a path for each image\n",
    "#     img_path = os.path.join(results_dir, f'generated_image_{i}.png')\n",
    "#     # Save the image\n",
    "#     save_image(img, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /home/harsh/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:06<00:00, 15.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: batch size is bigger than the data size. Setting batch size to data size\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Samples\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # The device to run the calculation on, 'cuda' or 'cpu'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# device = 'cuda'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the FID score\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m fid_value \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_fid_given_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFID score:\u001b[39m\u001b[38;5;124m'\u001b[39m, fid_value)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/pytorch_fid/fid_score.py:259\u001b[0m, in \u001b[0;36mcalculate_fid_given_paths\u001b[0;34m(paths, batch_size, device, dims, num_workers)\u001b[0m\n\u001b[1;32m    255\u001b[0m block_idx \u001b[38;5;241m=\u001b[39m InceptionV3\u001b[38;5;241m.\u001b[39mBLOCK_INDEX_BY_DIM[dims]\n\u001b[1;32m    257\u001b[0m model \u001b[38;5;241m=\u001b[39m InceptionV3([block_idx])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 259\u001b[0m m1, s1 \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_statistics_of_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m m2, s2 \u001b[38;5;241m=\u001b[39m compute_statistics_of_path(paths[\u001b[38;5;241m1\u001b[39m], model, batch_size,\n\u001b[1;32m    262\u001b[0m                                     dims, device, num_workers)\n\u001b[1;32m    263\u001b[0m fid_value \u001b[38;5;241m=\u001b[39m calculate_frechet_distance(m1, s1, m2, s2)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/pytorch_fid/fid_score.py:243\u001b[0m, in \u001b[0;36mcompute_statistics_of_path\u001b[0;34m(path, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    240\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path)\n\u001b[1;32m    241\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([file \u001b[38;5;28;01mfor\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m IMAGE_EXTENSIONS\n\u001b[1;32m    242\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ext))])\n\u001b[0;32m--> 243\u001b[0m     m, s \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_activation_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m, s\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/pytorch_fid/fid_score.py:228\u001b[0m, in \u001b[0;36mcalculate_activation_statistics\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_activation_statistics\u001b[39m(files, model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m    210\u001b[0m                                     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculation of the statistics used by the FID.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    Params:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    -- files       : List of image files paths\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m               the inception model.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     mu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(act, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    230\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(act, rowvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/pytorch_fid/fid_score.py:122\u001b[0m, in \u001b[0;36mget_activations\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    119\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(files)\n\u001b[1;32m    121\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ImagePathDataset(files, transforms\u001b[38;5;241m=\u001b[39mTF\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[0;32m--> 122\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m pred_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;28mlen\u001b[39m(files), dims))\n\u001b[1;32m    130\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:356\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    352\u001b[0m             sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     batch_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mBatchSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last \u001b[38;5;241m=\u001b[39m drop_last\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-cuda/lib/python3.12/site-packages/torch/utils/data/sampler.py:267\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sampler: Union[Sampler[\u001b[38;5;28mint\u001b[39m], Iterable[\u001b[38;5;28mint\u001b[39m]], batch_size: \u001b[38;5;28mint\u001b[39m, drop_last: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# check here.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m    266\u001b[0m             batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size should be a positive integer value, but got batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drop_last, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last should be a boolean value, but got drop_last=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_last\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=0"
     ]
    }
   ],
   "source": [
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "\n",
    "# Paths to the directories containing real and generated images\n",
    "paths = ['./Samples', './results']\n",
    "\n",
    "# # The device to run the calculation on, 'cuda' or 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "# Calculate the FID score\n",
    "fid_value = calculate_fid_given_paths(paths, batch_size=50, dims=2048,device=device)\n",
    "print('FID score:', fid_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
